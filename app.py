# app.py

import streamlit as st
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# --- ìƒìˆ˜ ì •ì˜ ---
CHROMA_DB_DIR = "chroma_db"

# --- RAG íŒŒì´í”„ë¼ì¸ ì„¤ì • ---

@st.cache_resource
def load_rag_pipeline():
    """RAG íŒŒì´í”„ë¼ì¸ì„ ë¡œë“œí•˜ê³  ìºì‹œí•©ë‹ˆë‹¤."""
    # 1. Vector DB ë¡œë“œ
    if not os.path.exists(CHROMA_DB_DIR):
        st.error(f"Vector DB í´ë”('{CHROMA_DB_DIR}')ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'indexer.py'ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        st.stop()
        
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vectordb = Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embeddings)
    
    # 2. Retriever ì„¤ì •
    retriever = vectordb.as_retriever(search_kwargs={"k": 3}) # ìƒìœ„ 3ê°œ ë¬¸ì„œë¥¼ ê²€ìƒ‰
    
    # 3. LLM ì„¤ì •
    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    
    # 4. Prompt í…œí”Œë¦¿ ì •ì˜
    prompt_template = """
    ë‹¹ì‹ ì€ í•™êµ ê³µì§€ì‚¬í•­ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ë§¥(context)ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ê·¼ê±° ìˆëŠ” ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
    ë‹µë³€ì€ ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:
    1. ë‹µë³€ì€ ë°˜ë“œì‹œ ì œê³µëœ 'ë¬¸ë§¥' ì •ë³´ì—ë§Œ ê¸°ë°˜í•´ì•¼ í•©ë‹ˆë‹¤. ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ ë‹µë³€ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
    2. ë‹µë³€ì´ ì–´ë–¤ ë¬¸ì„œì—ì„œ ë¹„ë¡¯ë˜ì—ˆëŠ”ì§€ 'ì¶œì²˜'ë¥¼ ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: [ì¶œì²˜: 2024ë…„ 1í•™ê¸° ì¥í•™ê¸ˆ ì‹ ì²­ ì•ˆë‚´.pdf])
    3. ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
    4. ë¬¸ë§¥ì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°, "ì£„ì†¡í•©ë‹ˆë‹¤, ì œê³µëœ ì •ë³´ ë‚´ì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ì†”ì§í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

    [ë¬¸ë§¥]
    {context}

    [ì§ˆë¬¸]
    {question}

    [ë‹µë³€]
    """
    
    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["context", "question"]
    )
    
    # 5. RetrievalQA ì²´ì¸ ìƒì„±
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )
    
    return qa_chain

# --- Streamlit UI ---

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ğŸ« í•™êµ ê³µì§€ì‚¬í•­ RAG ì±—ë´‡", page_icon="ğŸ¤–")

# ì œëª©
st.title("ğŸ« í•™êµ ê³µì§€ì‚¬í•­ RAG ì±—ë´‡")
st.caption("ê¶ê¸ˆí•œ í•™êµ ê³µì§€ì‚¬í•­ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")

# RAG íŒŒì´í”„ë¼ì¸ ë¡œë“œ
try:
    qa_chain = load_rag_pipeline()
except Exception as e:
    st.error(f"API í‚¤ ì„¤ì •ì— ë¬¸ì œê°€ ìˆëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”. ì˜¤ë¥˜: {e}")
    st.stop()


# ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! í•™êµ ê³µì§€ì‚¬í•­ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹ ê°€ìš”?"}]

# ì´ì „ ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
if prompt := st.chat_input("ì˜ˆ: 1í•™ê¸° êµ­ê°€ì¥í•™ê¸ˆ ì‹ ì²­ ê¸°ê°„ ì•Œë ¤ì¤˜"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ì±—ë´‡ ë‹µë³€ ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            try:
                response = qa_chain.invoke({"query": prompt})
                
                # ë‹µë³€ê³¼ ì¶œì²˜ë¥¼ ë¶„ë¦¬í•˜ì—¬ í‘œì‹œ
                answer = response['result']
                source_docs = response['source_documents']
                
                # ì¶œì²˜ ì •ë³´ ê°€ê³µ
                sources_text = ""
                if source_docs:
                    unique_sources = set()
                    for doc in source_docs:
                        title = doc.metadata.get('announcement_title', 'ì•Œ ìˆ˜ ì—†ëŠ” ê³µì§€')
                        filename = os.path.basename(doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼'))
                        unique_sources.add(f"'{title}' ê³µì§€ì˜ '{filename}' íŒŒì¼")
                    
                    sources_text = "\n\n---\n*ì°¸ê³  ìë£Œ:*\n- " + "\n- ".join(unique_sources)
                
                full_response = answer + sources_text
                st.markdown(full_response)
                
            except Exception as e:
                st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                full_response = "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    # ì±—ë´‡ ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "assistant", "content": full_response})