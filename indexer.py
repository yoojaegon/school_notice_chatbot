# indexer.py

import os
import logging
import logging.handlers
import datetime
from typing import List, Dict, Any, Sequence
import time

import chromadb
from dotenv import load_dotenv
from tqdm import tqdm

from preprocessing.handler import process_file

# ---- ìƒˆë¡œ ì¶”ê°€: OpenAI ì„ë² ë”© ì§ì ‘ í˜¸ì¶œ + ìºì‹œ ----
EMBED_MODEL = os.getenv("EMBED_MODEL", "text-embedding-3-small")
EMBED_BATCH_SIZE = int(os.getenv("EMBED_BATCH_SIZE", "128"))
EMBED_MAX_RETRIES = int(os.getenv("EMBED_MAX_RETRIES", "5"))
EMBED_BACKOFF_BASE = float(os.getenv("EMBED_BACKOFF_BASE", "1.5"))

# OpenAI SDK v1 (ê¶Œì¥)
try:
    from openai import OpenAI
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    _oai = OpenAI(api_key=OPENAI_API_KEY)
    def _embed_batch(model: str, texts: Sequence[str]) -> list[list[float]]:
        resp = _oai.embeddings.create(model=model, input=list(texts))
        return [d.embedding for d in resp.data]
except Exception:
    # v0.x í˜¸í™˜(ê°€ëŠ¥í•˜ë©´ v1ë¡œ ì—…ê·¸ë ˆì´ë“œ ê¶Œì¥)
    import openai
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    openai.api_key = OPENAI_API_KEY
    def _embed_batch(model: str, texts: Sequence[str]) -> list[list[float]]:
        resp = openai.Embedding.create(model=model, input=list(texts))
        return [d["embedding"] for d in resp["data"]]

from embed_cache import get_cached, set_cached


# --- ë¡œê¹… ì„¤ì • ---
def setup_logging():
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)
    log_file_path = os.path.join(log_dir, "indexer.log")

    logger = logging.getLogger()
    if logger.handlers:
        for h in logger.handlers[:]:
            logger.removeHandler(h)

    logger.setLevel(logging.DEBUG)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch.setFormatter(formatter)
    logger.addHandler(ch)

    fh = logging.handlers.RotatingFileHandler(
        log_file_path, maxBytes=5 * 1024 * 1024, backupCount=5, encoding='utf-8'
    )
    fh.setLevel(logging.DEBUG)
    fh.setFormatter(formatter)
    logger.addHandler(fh)

load_dotenv()

CRAWLED_DATA_DIR = "./crawled_data"
CHROMA_DB_DIR = "./chroma_db"
COLLECTION_NAME = "school_announcements"

EXTRACTED_SAVE_DIR = os.getenv("EXTRACTED_SAVE_DIR", "./extracted_data")
SAVE_EXTRACTED_TEXT = os.getenv("SAVE_EXTRACTED_TEXT", "true").lower() in ("1", "true", "yes", "y")

log = logging.getLogger(__name__)


def _sanitize_filename(name: str) -> str:
    cleaned = "".join(c for c in name if c not in '\\/*?:"<>|').strip()
    return cleaned or "untitled"

def get_post_id_from_path(file_path: str) -> str:
    try:
        post_directory_path = os.path.dirname(file_path)
        post_folder_name = os.path.basename(post_directory_path)
        post_id = post_folder_name.split('_', 1)[0]
        if post_id.isdigit():
            return post_id
        else:
            log.debug(f"ê²½ë¡œì—ì„œ ìˆ«ì í˜•ì‹ì˜ post_idë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {file_path}")
            return "unknown"
    except IndexError:
        log.warning(f"ê²½ë¡œ êµ¬ì¡°ê°€ ì˜ˆìƒê³¼ ë‹¬ë¼ post_idë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return "unknown"

def _normalize_category(raw: str) -> str:
    if not raw:
        return "chunk"
    raw_lower = str(raw).lower()
    if raw_lower in ("post_content", "content", "post", "post_body"):
        return "post_content"
    if raw_lower in ("attachment_text", "attachment", "file", "image_attachment_ocr", "ocr_image_attachment", "attachment_body"):
        return "attachment_text"
    if raw_lower in ("embedded_image_text", "embedded_image", "image_in_document", "image_embedded_ocr", "embedded_image_ocr"):
        return "embedded_image_text"
    return _sanitize_filename(raw_lower)

def _save_chunks_to_disk(post_id: str, file_path: str, structured_chunks: List[Dict[str, Any]]) -> None:
    if not SAVE_EXTRACTED_TEXT:
        return
    try:
        base_filename = os.path.basename(file_path)
        for i, chunk in enumerate(structured_chunks):
            meta = dict(chunk.get("metadata", {}))
            category = _normalize_category(meta.get("category") or meta.get("type") or "chunk")
            out_dir = os.path.join(EXTRACTED_SAVE_DIR, post_id, category)
            os.makedirs(out_dir, exist_ok=True)
            out_name = f"chunk_{i:03d}__{_sanitize_filename(base_filename)}.txt"
            out_path = os.path.join(out_dir, out_name)
            header_lines = [
                f"# saved_at: {datetime.datetime.now().isoformat(timespec='seconds')}",
                f"# post_id: {post_id}",
                f"# category: {category}",
                f"# source_file: {file_path}",
                f"# title: {meta.get('announcement_title', '')}",
                f"# source: {meta.get('source', '')}",
                f"# extra: board={meta.get('board_name', '')}, post_url={meta.get('post_url', '')}",
                "#" + "-" * 78,
                "",
            ]
            body = chunk.get("page_content", "")
            with open(out_path, "w", encoding="utf-8") as f:
                f.write("\n".join(header_lines))
                f.write(body)
        log.info(f"ğŸ“ ì¶”ì¶œ í…ìŠ¤íŠ¸ {len(structured_chunks)}ê°œë¥¼ ë””ìŠ¤í¬ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤: {os.path.join(EXTRACTED_SAVE_DIR, post_id)}")
    except Exception:
        log.error("ì¶”ì¶œ í…ìŠ¤íŠ¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ", exc_info=True)

# ---- ì„ë² ë”© ë°°ì¹˜ + ìºì‹œ ----
def embed_in_batches(texts: Sequence[str],
                     model: str = EMBED_MODEL,
                     batch_size: int = EMBED_BATCH_SIZE,
                     max_retries: int = EMBED_MAX_RETRIES,
                     backoff_base: float = EMBED_BACKOFF_BASE) -> list[list[float]]:
    vectors: list[list[float]] = []
    n = len(texts)
    for start in range(0, n, batch_size):
        chunk = texts[start:start+batch_size]

        # ìºì‹œ í™•ì¸
        cached_vecs: list[list[float]] = []
        to_call_idx, to_call_texts = [], []
        for i, t in enumerate(chunk):
            c = get_cached(t, model)
            if c is not None:
                cached_vecs.append(c)
            else:
                cached_vecs.append(None)
                to_call_idx.append(i)
                to_call_texts.append(t)

        # API í˜¸ì¶œ(í•„ìš”í•œ ê²ƒë§Œ)
        if to_call_texts:
            for attempt in range(max_retries):
                try:
                    api_vecs = _embed_batch(model, to_call_texts)
                    # ê²°ê³¼ ë§¤í•‘
                    for offset, v in zip(to_call_idx, api_vecs):
                        cached_vecs[offset] = v
                        set_cached(chunk[offset], model, v)
                    break
                except Exception as e:
                    wait = (backoff_base ** attempt) + 0.1 * attempt
                    log.warning(f"ì„ë² ë”© ë°°ì¹˜ ì‹¤íŒ¨({attempt+1}/{max_retries}): {e} -> {wait:.1f}s ëŒ€ê¸° í›„ ì¬ì‹œë„")
                    time.sleep(wait)
            else:
                raise RuntimeError("ì„ë² ë”© ë°°ì¹˜ ê³„ì‚°ì— ë°˜ë³µì ìœ¼ë¡œ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

        # ë°°ì¹˜ ê²°ê³¼ í•©ì¹˜ê¸°
        vectors.extend(cached_vecs)
        log.info(f"ì„ë² ë”© ì²˜ë¦¬: {start} ~ {start+len(chunk)-1} (ìºì‹œ {len(chunk)-len(to_call_texts)}/{len(chunk)})")

    return vectors


def main():
    log.info("=" * 50)
    log.info("ë°ì´í„° ìƒ‰ì¸ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    log.info("=" * 50)

    # 1) ChromaDB í´ë¼ì´ì–¸íŠ¸ (âš  ì„ë² ë”© í•¨ìˆ˜ ì œê±°)
    log.info("ChromaDB í´ë¼ì´ì–¸íŠ¸ ì„¤ì •...")
    try:
        client = chromadb.PersistentClient(path=CHROMA_DB_DIR)
        collection = client.get_or_create_collection(
            name=COLLECTION_NAME,
            metadata={"hnsw:space": "cosine"},
        )
        log.info(f"ChromaDB ì»¬ë ‰ì…˜ '{COLLECTION_NAME}' ì¤€ë¹„ ì™„ë£Œ.")
    except Exception:
        log.critical("ChromaDB ì„¤ì • ì¤‘ ì˜¤ë¥˜. ê²½ë¡œ/ê¶Œí•œ í™•ì¸ ìš”ë§.", exc_info=True)
        return

    # 2) ì²˜ë¦¬ ëŒ€ìƒ íŒŒì¼ ìˆ˜ì§‘
    if not os.path.exists(CRAWLED_DATA_DIR):
        log.error(f"'{CRAWLED_DATA_DIR}' ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 'crawler.py' ì‹¤í–‰ í•„ìš”.")
        return

    files_to_process: List[str] = []
    for root, _, filenames in os.walk(CRAWLED_DATA_DIR):
        for filename in filenames:
            if not filename.startswith('.'):
                files_to_process.append(os.path.join(root, filename))

    if not files_to_process:
        log.warning(f"'{CRAWLED_DATA_DIR}' ë””ë ‰í† ë¦¬ì— ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    log.info(f"ì´ {len(files_to_process)}ê°œì˜ íŒŒì¼ì„ ì²˜ë¦¬ ëŒ€ìƒìœ¼ë¡œ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

    # 3) íŒŒì¼ ì²˜ë¦¬
    for file_path in tqdm(files_to_process, desc="íŒŒì¼ ìƒ‰ì¸ ì¤‘"):
        log.debug(f"--- íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file_path} ---")

        post_id = get_post_id_from_path(file_path)
        if post_id == "unknown":
            log.warning(f"ê²Œì‹œë¬¼ IDë¥¼ ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼ì€ ê±´ë„ˆëœë‹ˆë‹¤: {file_path}")
            continue

        structured_chunks = process_file(file_path)
        if not structured_chunks:
            log.warning(f"ì²˜ë¦¬í•  ì²­í¬ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {os.path.basename(file_path)}")
            continue

        ids, documents, metadatas = [], [], []
        base_filename = os.path.basename(file_path)

        for i, chunk_data in enumerate(structured_chunks):
            meta = dict(chunk_data.get("metadata", {}))
            meta["post_id"] = post_id
            chunk_id = f"{post_id}_{base_filename}_{meta.get('category', meta.get('type', 'chunk'))}_{i}"

            ids.append(chunk_id)
            documents.append(chunk_data.get("page_content", ""))
            metadatas.append(meta)

        # ---- ì—¬ê¸°ì„œë¶€í„° í•µì‹¬: ì„ë² ë”©ì„ ì§ì ‘ ë°°ì¹˜ ê³„ì‚°í•˜ê³  ë°”ë¡œ ë„£ê¸° ----
        try:
            log.info(f"ì„ë² ë”© ê³„ì‚° ì‹œì‘ (ë¬¸ì„œ ìˆ˜: {len(documents)})")
            embeddings = embed_in_batches(documents, model=EMBED_MODEL, batch_size=EMBED_BATCH_SIZE)
            # upsert: ê°™ì€ IDë©´ ë®ì–´ì“°ê¸° (ì¬ì‹¤í–‰ ì•ˆì „)
            collection.upsert(ids=ids, documents=documents, metadatas=metadatas, embeddings=embeddings)
            log.info(f"âœ… ì„±ê³µ: {len(ids)}ê°œì˜ ì²­í¬(post_id: {post_id})ë¥¼ DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        except Exception:
            log.error(f"âŒ ì‹¤íŒ¨: '{file_path}' ì„ë² ë”©/DB ì €ì¥ ì¤‘ ì˜¤ë¥˜", exc_info=True)
            continue

        # (ì„ íƒ) ë””ìŠ¤í¬ ì €ì¥
        try:
            _save_chunks_to_disk(post_id, file_path, structured_chunks)
        except Exception:
            log.error("ë””ìŠ¤í¬ ì €ì¥ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜", exc_info=True)

    log.info("=" * 50)
    log.info("ëª¨ë“  íŒŒì¼ì˜ ìƒ‰ì¸ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    log.info(f"ìµœì¢… ë°ì´í„°ëŠ” '{CHROMA_DB_DIR}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    if SAVE_EXTRACTED_TEXT:
        log.info(f"ì¶”ì¶œ í…ìŠ¤íŠ¸ëŠ” '{EXTRACTED_SAVE_DIR}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    log.info("=" * 50)

if __name__ == "__main__":
    setup_logging()
    main()